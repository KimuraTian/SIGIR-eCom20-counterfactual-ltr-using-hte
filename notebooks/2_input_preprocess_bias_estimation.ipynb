{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Processing for Propensity Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal forests and X Learner input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'Fold1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_exp_train_vali_rankings = pd.read_csv(f'../build/simulation/{fold}/sim_exp_train_vali_rankings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nqids = sim_exp_train_vali_rankings['qid'].nunique()\n",
    "unique_qids = sim_exp_train_vali_rankings['qid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
       "       261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "       274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "       287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
       "       300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "       352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "       365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "       378, 379, 380, 381])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 38, 190, 381]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nqids // i for i in [100, 10, 2, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nq in [nqids // i for i in [100, 10, 2, 1]]:\n",
    "    sampled_qids = np.random.choice(unique_qids, size=nq, replace=False)\n",
    "    with open(f'../build/simulation/{fold}/sim_exp_swap_query_ids_{nq}.pkl', 'wb') as f:\n",
    "        pickle.dump(sampled_qids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examination_features = pd.read_csv(f'../build/simulation/{fold}/examination_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_one_intervention(data, arms):\n",
    "    control_and_treatment_data = data[data['arm'].isin(arms) & data['swapped_rank'].isin(arms)]\n",
    "    observed_intervention = control_and_treatment_data[['qd_id', 'swapped_rank']].drop_duplicates()\n",
    "    sampled_intervention = observed_intervention.groupby('qd_id', as_index=False, group_keys=False).apply(lambda x: x.sample(n=1))\n",
    "    sampled_intervention['keep'] = 'keep'\n",
    "    sampled_cases = control_and_treatment_data.merge(sampled_intervention, how='left', on=['qd_id', 'swapped_rank']).dropna(subset=['keep']).drop('keep', axis=1)\n",
    "    sampled_cases['treatment'] = (sampled_cases['swapped_rank'] == 1).astype(int)\n",
    "    return sampled_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading swap clicks with 5 sessions\n",
      "Loading sampled qids with sample size 3\n",
      "Loaded 3 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 38\n",
      "Loaded 38 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 190\n",
      "Loaded 190 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 381\n",
      "Loaded 381 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading swap clicks with 10 sessions\n",
      "Loading sampled qids with sample size 3\n",
      "Loaded 3 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 38\n",
      "Loaded 38 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 190\n",
      "Loaded 190 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 381\n",
      "Loaded 381 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading swap clicks with 25 sessions\n",
      "Loading sampled qids with sample size 3\n",
      "Loaded 3 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 38\n",
      "Loaded 38 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 190\n",
      "Loaded 190 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 381\n",
      "Loaded 381 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading swap clicks with 50 sessions\n",
      "Loading sampled qids with sample size 3\n",
      "Loaded 3 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 38\n",
      "Loaded 38 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 190\n",
      "Loaded 190 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n",
      "Loading sampled qids with sample size 381\n",
      "Loaded 381 unique queries\n",
      "Sampling one of two intervention pairs for each query document pair\n",
      "Saving random pairs\n"
     ]
    }
   ],
   "source": [
    "for avg_clicks in [5, 10, 25, 50]:\n",
    "    print(f'Loading swap clicks with {avg_clicks} sessions')\n",
    "    swap_clicks = pd.read_csv(f'../build/simulation/{fold}/sim_exp_swap_train_vali_clicks_avg_clicks_{avg_clicks}.csv', low_memory=False)\n",
    "    for nq in [nqids // i for i in [100, 10, 2, 1]]:\n",
    "        print(f'Loading sampled qids with sample size {nq}')\n",
    "        with open(f'../build/simulation/{fold}/sim_exp_swap_query_ids_{nq}.pkl', 'rb') as f:\n",
    "            sampled_qids = pickle.load(f)\n",
    "        print(f'Loaded {sampled_qids.shape[0]} unique queries')\n",
    "        sample_clicks = swap_clicks[swap_clicks['qid'].isin(sampled_qids)]\n",
    "        sample_clicks_arms = sample_clicks.loc[sample_clicks['pred_rank'] == 1, ['list_id', 'swapped_rank']].rename(columns={'swapped_rank': 'arm'})\n",
    "        sample_clicks = sample_clicks.merge(sample_clicks_arms, how='left')\n",
    "        print('Sampling one of two intervention pairs for each query document pair')\n",
    "        randomized_ctr_list = []\n",
    "        for i in range(2, 11):\n",
    "            randomized_clicks = random_sample_one_intervention(sample_clicks, (1, i))\n",
    "            meta_data = randomized_clicks[['partition', 'qd_id', 'true_click_probability', 'true_propensity']].drop_duplicates()\n",
    "            randomized_ctr = randomized_clicks.groupby(['partition', 'qd_id', 'swapped_rank'])[['click', 'treatment']].mean().reset_index()\n",
    "            randomized_ctr = randomized_ctr.merge(meta_data, how='left', on=['partition', 'qd_id'])\n",
    "            randomized_ctr['treatment_group'] = i\n",
    "            randomized_ctr_list.append(randomized_ctr)\n",
    "        randomized_ctr = pd.concat(randomized_ctr_list, ignore_index=True)\n",
    "        randomized_ctr = randomized_ctr.merge(examination_features, on=['partition', 'qd_id'], how='left')\n",
    "        print('Saving random pairs')\n",
    "        randomized_ctr.to_csv(f'../build/simulation/{fold}/sim_exp_swap_causal_forests_train_clicks_{avg_clicks}_{nq}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An existing Contextual Position Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading examination feature columns\n"
     ]
    }
   ],
   "source": [
    "with open(f'../build/simulation/{fold}/examination_fc_names.pkl', 'rb') as f:\n",
    "    print(f'Loading examination feature columns')\n",
    "    examination_fc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['53', '54', '63', '99', '103', '106', '108', '126', '129', '133'],\n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examination_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading swap clicks with 5 avg clicks\n",
      "Loading sampled qids with sample size 3\n",
      "Loaded 3 unique queries\n",
      "Saving cpbm clicks 5; 3\n",
      "Loading sampled qids with sample size 38\n",
      "Loaded 38 unique queries\n",
      "Saving cpbm clicks 5; 38\n",
      "Loading sampled qids with sample size 190\n",
      "Loaded 190 unique queries\n",
      "Saving cpbm clicks 5; 190\n",
      "Loading sampled qids with sample size 381\n",
      "Loaded 381 unique queries\n",
      "Saving cpbm clicks 5; 381\n",
      "Loading swap clicks with 10 avg clicks\n",
      "Loading sampled qids with sample size 3\n",
      "Loaded 3 unique queries\n",
      "Saving cpbm clicks 10; 3\n",
      "Loading sampled qids with sample size 38\n",
      "Loaded 38 unique queries\n",
      "Saving cpbm clicks 10; 38\n",
      "Loading sampled qids with sample size 190\n",
      "Loaded 190 unique queries\n",
      "Saving cpbm clicks 10; 190\n",
      "Loading sampled qids with sample size 381\n",
      "Loaded 381 unique queries\n",
      "Saving cpbm clicks 10; 381\n",
      "Loading swap clicks with 25 avg clicks\n",
      "Loading sampled qids with sample size 3\n",
      "Loaded 3 unique queries\n",
      "Saving cpbm clicks 25; 3\n",
      "Loading sampled qids with sample size 38\n",
      "Loaded 38 unique queries\n",
      "Saving cpbm clicks 25; 38\n",
      "Loading sampled qids with sample size 190\n",
      "Loaded 190 unique queries\n",
      "Saving cpbm clicks 25; 190\n",
      "Loading sampled qids with sample size 381\n",
      "Loaded 381 unique queries\n",
      "Saving cpbm clicks 25; 381\n",
      "Loading swap clicks with 50 avg clicks\n",
      "Loading sampled qids with sample size 3\n",
      "Loaded 3 unique queries\n",
      "Saving cpbm clicks 50; 3\n",
      "Loading sampled qids with sample size 38\n",
      "Loaded 38 unique queries\n",
      "Saving cpbm clicks 50; 38\n",
      "Loading sampled qids with sample size 190\n",
      "Loaded 190 unique queries\n",
      "Saving cpbm clicks 50; 190\n",
      "Loading sampled qids with sample size 381\n",
      "Loaded 381 unique queries\n",
      "Saving cpbm clicks 50; 381\n"
     ]
    }
   ],
   "source": [
    "for avg_clicks in [5, 10, 25, 50]:\n",
    "    print(f'Loading swap clicks with {avg_clicks} avg clicks')\n",
    "    swap_clicks = pd.read_csv(f'../build/simulation/{fold}/sim_exp_swap_train_vali_clicks_avg_clicks_{avg_clicks}.csv', low_memory=False)\n",
    "    swap_clicks['rank_idx'] = swap_clicks['swapped_rank'].astype(int) - 1\n",
    "    for nq in [nqids // i for i in [100, 10, 2, 1]]:\n",
    "        print(f'Loading sampled qids with sample size {nq}')\n",
    "        with open(f'../build/simulation/{fold}/sim_exp_swap_query_ids_{nq}.pkl', 'rb') as f:\n",
    "            sampled_qids = pickle.load(f)\n",
    "        print(f'Loaded {sampled_qids.shape[0]} unique queries')\n",
    "        # sample clicks and number rows\n",
    "        sample_clicks = swap_clicks[swap_clicks['qid'].isin(sampled_qids)].copy()\n",
    "        sample_clicks['click_idx'] = np.arange(sample_clicks.shape[0])\n",
    "        # query document pair frequency for each rank\n",
    "        doc_rank_count = sample_clicks.groupby('qd_id')['rank_idx'].value_counts().rename('doc_rank_count').reset_index()\n",
    "        # unique ranks for each unique query document pair\n",
    "        doc_intervention_ranks = sample_clicks[['qd_id', 'rank_idx']].drop_duplicates().rename(columns={'rank_idx': 'intervention_rank_idx'})\n",
    "        doc_stats = doc_rank_count.merge(doc_intervention_ranks, on=['qd_id']).sort_values(['qd_id', 'rank_idx', 'intervention_rank_idx'])\n",
    "        intervention_set = doc_stats[doc_stats['rank_idx'] != doc_stats['intervention_rank_idx']]\n",
    "        # merge clicks with intervention set\n",
    "        cpbm_train_click = sample_clicks.merge(intervention_set, on=['qd_id', 'rank_idx'], how='left').dropna(subset=['doc_rank_count'])\n",
    "        # negative labels\n",
    "        cpbm_train_click['neg_click'] = 1 - cpbm_train_click['click']\n",
    "        # inverse assignment frequency weighted labels\n",
    "        cpbm_train_click['inverse_frequency_weighted_pos_click'] = cpbm_train_click['click'] / cpbm_train_click['doc_rank_count']\n",
    "        cpbm_train_click['inverse_frequency_weighted_neg_click'] = cpbm_train_click['neg_click'] / cpbm_train_click['doc_rank_count']\n",
    "        # reindex rows\n",
    "        cpbm_train_click['click_idx'] = cpbm_train_click.groupby('click_idx').ngroup()\n",
    "        cpbm_train_click = cpbm_train_click.sort_values(['click_idx', 'rank_idx', 'intervention_rank_idx'])\n",
    "        print(f'Saving cpbm clicks {avg_clicks}; {nq}')\n",
    "        cpbm_train_click.to_csv(f'../build/simulation/{fold}/sim_exp_swap_cpbm_train_clicks_{avg_clicks}_{nq}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
